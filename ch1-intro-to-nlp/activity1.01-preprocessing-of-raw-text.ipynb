{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42432cae-a215-4649-87bc-babcfaac8528",
   "metadata": {},
   "source": [
    "### Activity 1.01: Preprocessing of Raw Text\n",
    "\n",
    "We have a text corpus that is in an improper format. In this activity, we will perform all the preprocessing steps that were discussed earlier to get some meaning out of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa1808b2-ef1f-4bee-b0a5-027c06b814bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/LNonyane/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/LNonyane/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/LNonyane/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Required libraries\n",
    "from nltk import word_tokenize, download, pos_tag, stem, sent_tokenize\n",
    "download(['punkt', 'averaged_perceptron_tagger', 'stopwords'])\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from autocorrect import Speller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3193aac9-518b-42b5-844e-59f1110516f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text corpus to a variable.\n",
    "text = \"The reader of this course should have a basic knowledge of the Python programming lenguage.\\\n",
    " He/she must have knowldge of data types in Python. He should be able to write functions,\\\n",
    " and also have the ability to import and use libraries and packages in Python. Familiarity\\\n",
    " with basic linguistics and probability is assumed although not required to fully\\\n",
    " complete this course.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f07dbbc-c82e-461f-9d9d-80d580ebce0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reader of this course should have a basic knowledge of the Python programming lenguage. He/she must have knowldge of data types in Python. He should be able to write functions, and also have the ability to import and use libraries and packages in Python. Familiarity with basic linguistics and probability is assumed although not required to fully complete this course.\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "571a214a-af76-474c-87a5-784e83fc1f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'reader', 'of', 'this', 'course', 'should', 'have', 'a', 'basic', 'knowledge', 'of', 'the', 'Python', 'programming', 'lenguage', '.', 'He/she', 'must', 'have', 'knowldge']\n"
     ]
    }
   ],
   "source": [
    "# Apply the tokenization process to the text corpus and print the first 20 tokens.\n",
    "#sentence = sent_tokenize(text)\n",
    "word_token = word_tokenize(text)\n",
    "print(word_token[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "586ba54f-ae3b-4931-85db-7ef9cd9ba391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply spelling correction on each token and print the initial 20 corrected tokens as well as the corrected text corpus.\n",
    "spell = Speller(lang='en')\n",
    "def correct_spelling(tokens):\n",
    "    sentence_corrected = ' '.join([spell(word) for word in tokens])\n",
    "    tokens_corrected = [spell(word) for word in tokens]\n",
    "    return sentence_corrected, tokens_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a545f4c-f343-4a8d-b798-056370c7cea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reader of this course should have a basic knowledge of the Python programming language . He/she must have knowledge of data types in Python . He should be able to write functions , and also have the ability to import and use libraries and packages in Python . Familiarity with basic linguistics and probability is assumed although not required to fully complete this course .\n",
      "['The', 'reader', 'of', 'this', 'course', 'should', 'have', 'a', 'basic', 'knowledge', 'of', 'the', 'Python', 'programming', 'language', '.', 'He/she', 'must', 'have', 'knowledge', 'of', 'data', 'types', 'in', 'Python', '.', 'He', 'should', 'be', 'able', 'to', 'write', 'functions', ',', 'and', 'also', 'have', 'the', 'ability', 'to', 'import', 'and', 'use', 'libraries', 'and', 'packages', 'in', 'Python', '.', 'Familiarity', 'with', 'basic', 'linguistics', 'and', 'probability', 'is', 'assumed', 'although', 'not', 'required', 'to', 'fully', 'complete', 'this', 'course', '.']\n"
     ]
    }
   ],
   "source": [
    "word_token, word_token_correct = correct_spelling(word_token)\n",
    "print(word_token)\n",
    "print(word_token_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56d314ee-7c78-4745-b30d-840b625e8b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PoS tags to each of the corrected tokens and print them.\n",
    "def get_pos(tokens):\n",
    "    tags = pos_tag(tokens)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e8dbc39-dc4b-485b-bc75-81c92a45adfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('reader', 'NN'), ('of', 'IN'), ('this', 'DT'), ('course', 'NN'), ('should', 'MD'), ('have', 'VB'), ('a', 'DT'), ('basic', 'JJ'), ('knowledge', 'NN'), ('of', 'IN'), ('the', 'DT'), ('Python', 'NNP'), ('programming', 'NN'), ('language', 'NN'), ('.', '.'), ('He/she', 'NNP'), ('must', 'MD'), ('have', 'VB'), ('knowledge', 'NN'), ('of', 'IN'), ('data', 'NNS'), ('types', 'NNS'), ('in', 'IN'), ('Python', 'NNP'), ('.', '.'), ('He', 'PRP'), ('should', 'MD'), ('be', 'VB'), ('able', 'JJ'), ('to', 'TO'), ('write', 'VB'), ('functions', 'NNS'), (',', ','), ('and', 'CC'), ('also', 'RB'), ('have', 'VBP'), ('the', 'DT'), ('ability', 'NN'), ('to', 'TO'), ('import', 'NN'), ('and', 'CC'), ('use', 'NN'), ('libraries', 'NNS'), ('and', 'CC'), ('packages', 'NNS'), ('in', 'IN'), ('Python', 'NNP'), ('.', '.'), ('Familiarity', 'NN'), ('with', 'IN'), ('basic', 'JJ'), ('linguistics', 'NNS'), ('and', 'CC'), ('probability', 'NN'), ('is', 'VBZ'), ('assumed', 'VBN'), ('although', 'IN'), ('not', 'RB'), ('required', 'VBN'), ('to', 'TO'), ('fully', 'RB'), ('complete', 'VB'), ('this', 'DT'), ('course', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "pos_tags = get_pos(word_token_correct)\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e939564-d0df-4590-8c75-0a3315dd7cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words from the corrected token list and print the initial 20 tokens.\n",
    "stop_words = stopwords.words('english') # English stopwords\n",
    "stop_punc = list(punctuation)\n",
    "stop_final = stop_words + stop_punc\n",
    "def remove_stopwords(tokens, stop_final):\n",
    "    return [token for token in tokens if token not in stop_final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bc9890a-1c9a-40a3-a142-dc11694ce7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'reader', 'course', 'basic', 'knowledge', 'Python', 'programming', 'language', 'He/she', 'must', 'knowledge', 'data', 'types', 'Python', 'He', 'able', 'write', 'functions', 'also', 'ability']\n"
     ]
    }
   ],
   "source": [
    "tokens_no_stop_words = remove_stopwords(word_token_correct, stop_final)\n",
    "print(tokens_no_stop_words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf046914-4030-4c74-afac-da2ee000314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stemming and lemmatization to the corrected token list and the print the initial 20 tokens.\n",
    "def get_stems(tokens, stemmer):\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "porterStem = stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f453678-5945-4f10-8f07-7249bfff3933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'reader',\n",
       " 'cours',\n",
       " 'basic',\n",
       " 'knowledg',\n",
       " 'python',\n",
       " 'program',\n",
       " 'languag',\n",
       " 'he/sh',\n",
       " 'must',\n",
       " 'knowledg',\n",
       " 'data',\n",
       " 'type',\n",
       " 'python',\n",
       " 'he',\n",
       " 'abl',\n",
       " 'write',\n",
       " 'function',\n",
       " 'also',\n",
       " 'abil']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_stems = get_stems(tokens_no_stop_words, porterStem)\n",
    "token_stems[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f425ced-5745-46fe-b325-7862dc2971c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = stem.WordNetLemmatizer()\n",
    "def get_lemma(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d167c0b-2e4c-4350-b35c-edce4b80614b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'reader',\n",
       " 'course',\n",
       " 'basic',\n",
       " 'knowledge',\n",
       " 'Python',\n",
       " 'programming',\n",
       " 'language',\n",
       " 'He/she',\n",
       " 'must',\n",
       " 'knowledge',\n",
       " 'data',\n",
       " 'type',\n",
       " 'Python',\n",
       " 'He',\n",
       " 'able',\n",
       " 'write',\n",
       " 'function',\n",
       " 'also',\n",
       " 'ability']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_lemma = get_lemma(tokens_no_stop_words)\n",
    "token_lemma[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5638f2a-58be-4bb3-9a29-b215f989a508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The reader of this course should have a basic knowledge of the Python programming lenguage.', 'He/she must have knowldge of data types in Python.', 'He should be able to write functions, and also have the ability to import and use libraries and packages in Python.', 'Familiarity with basic linguistics and probability is assumed although not required to fully complete this course.']\n"
     ]
    }
   ],
   "source": [
    "# Detect the sentence boundaries in the given text corpus and print the total number of sentences.\n",
    "sentence = sent_tokenize(text)\n",
    "print(sentence) # total number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ecf39d-a400-4b7d-9990-685799979c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
